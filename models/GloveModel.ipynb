{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a665938-4fc7-4100-ba4f-ab447429ea04",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "s3 = boto3.client(\"s3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60391e51-54a2-440b-a1e5-1b65e3826003",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "bucket_name = \"s3-cyberbullying-classification-data\"\n",
    "\n",
    "# Lista objetos en el bucket\n",
    "response = s3.list_objects_v2(Bucket=bucket_name)\n",
    "\n",
    "# Mostrar los nombres de los archivos\n",
    "if \"Contents\" in response:\n",
    "    for obj in response[\"Contents\"]:\n",
    "        print(obj[\"Key\"])\n",
    "else:\n",
    "    print(\"No se encontraron archivos o no tienes permisos\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2eaa94bb-c74f-4e6a-9ab1-8ae0329913c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip uninstall -y spacy\n",
    "!pip install spacy==3.7.2\n",
    "!pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.sparse import hstack\n",
    "import spacy\n",
    "\n",
    "# ==============================\n",
    "# 1. Cargar datos\n",
    "# ==============================\n",
    "key = \"files/md5/c8/5e2d40bf87b27619f2a4c49fcb9cda\"\n",
    "obj = s3.get_object(Bucket=bucket_name, Key=key)\n",
    "\n",
    "data = pd.read_csv(obj[\"Body\"])\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# 2. Preprocesamiento simple\n",
    "# ==============================\n",
    "def clean_tweet(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    text = re.sub(r\"@\\w+\", \"\", text)\n",
    "    text = re.sub(r\"#\", \"\", text)\n",
    "    text = re.sub(r\"[^a-záéíóúüñ\\s]\", \"\", text)\n",
    "    return text.strip()\n",
    "\n",
    "data[\"clean_text\"] = data[\"tweet_text\"].astype(str).apply(clean_tweet)\n",
    "print(\"[LOG] Preprocesamiento completado. Ejemplo:\", data[\"clean_text\"].iloc[0])\n",
    "\n",
    "# ==============================\n",
    "# 3. TF-IDF por palabras (unigramas y bigramas)\n",
    "# ==============================\n",
    "word_vectorizer = TfidfVectorizer(\n",
    "    max_features=7500,\n",
    "    ngram_range=(1, 2),\n",
    "    stop_words=\"english\"\n",
    ")\n",
    "X_tfidf_words = word_vectorizer.fit_transform(data[\"clean_text\"])\n",
    "print(\"[LOG] TF-IDF de palabras listo. Dimensión:\", X_tfidf_words.shape)\n",
    "\n",
    "# ==============================\n",
    "# 4. TF-IDF por caracteres (3–5-gramas)\n",
    "# ==============================\n",
    "char_vectorizer = TfidfVectorizer(\n",
    "    analyzer=\"char\",\n",
    "    ngram_range=(3, 5),\n",
    "    max_features=500\n",
    ")\n",
    "X_tfidf_chars = char_vectorizer.fit_transform(data[\"clean_text\"])\n",
    "print(\"[LOG] TF-IDF de caracteres listo. Dimensión:\", X_tfidf_chars.shape)\n",
    "\n",
    "# ==============================\n",
    "# 5. Embeddings densos con spaCy\n",
    "# ==============================\n",
    "!python -m spacy download en_core_web_smimport spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "def get_glove_embedding(text, embedding_dim=96):  # en_core_web_sm -> 96 dims\n",
    "    doc = nlp(text)\n",
    "    return doc.vector if doc.has_vector else np.zeros(embedding_dim)\n",
    "\n",
    "\n",
    "X_glove = np.array([get_glove_embedding(t) for t in data[\"clean_text\"]])\n",
    "print(\"[LOG] Embeddings GloVe generados. Dimensión:\", X_glove.shape)\n",
    "\n",
    "# Escalar embeddings densos\n",
    "scaler = StandardScaler()\n",
    "X_glove_scaled = scaler.fit_transform(X_glove)\n",
    "print(\"[LOG] Escalado de embeddings completado.\")\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# 6. Combinar todo\n",
    "# ==============================\n",
    "X_combined = hstack([X_tfidf_words, X_tfidf_chars, 2 * X_glove_scaled])\n",
    "y = data[\"cyberbullying_type\"]\n",
    "print(\"[LOG] Matriz final lista. Dimensión:\", X_combined.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47ff004f-5f1a-4398-8076-5b07321b2d51",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "joblib.dump(X_combined, \"X_features.pkl\")\n",
    "joblib.dump(y, \"y_labels.pkl\")\n",
    "joblib.dump(word_vectorizer, \"tfidf_vectorizer.pkl\")\n",
    "joblib.dump(char_vectorizer, \"char_vectorizer.pkl\")\n",
    "joblib.dump(scaler, \"scaler.pkl\")\n",
    "\n",
    "print(\"[LOG] Features y etiquetas guardadas exitosamente.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7afc0f8f-14c7-4b1d-a960-01c4a2dca1ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!pip install xgboost\n",
    "import mlflow\n",
    "import mlflow.xgboost\n",
    "import joblib\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import recall_score\n",
    "from xgboost import XGBClassifier\n",
    "import itertools\n",
    "import time\n",
    "\n",
    "# ==============================\n",
    "# 1. Cargar features y etiquetas\n",
    "# ==============================\n",
    "X = joblib.load(\"X_features.pkl\")\n",
    "y = joblib.load(\"y_labels.pkl\")\n",
    "print(\"[LOG] Features y etiquetas cargadas:\", X.shape, len(y))\n",
    "\n",
    "# ==============================\n",
    "# 2. Codificar etiquetas\n",
    "# ==============================\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "print(\"[LOG] Etiquetas codificadas:\", np.unique(y_encoded))\n",
    "\n",
    "# ==============================\n",
    "# 3. Dividir en train/test\n",
    "# ==============================\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_encoded, test_size=0.2, random_state=55\n",
    ")\n",
    "print(\"[LOG] División train/test:\", X_train.shape, X_test.shape)\n",
    "\n",
    "# ==============================\n",
    "# 4. Definir grid de hiperparámetros\n",
    "# ==============================\n",
    "param_grid = {\n",
    "    \"max_depth\": [5, 7],\n",
    "    \"learning_rate\": [0.01, 0.005],\n",
    "    \"n_estimators\": [100, 200],\n",
    "    \"subsample\": [0.8, 0.9]\n",
    "}\n",
    "\n",
    "# Generar todas las combinaciones posibles\n",
    "keys, values = zip(*param_grid.items())\n",
    "param_combinations = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
    "print(f\"[LOG] Total de combinaciones: {len(param_combinations)}\")\n",
    "\n",
    "# ==============================\n",
    "# 5. Configurar experimento MLflow\n",
    "# ==============================\n",
    "mlflow.set_experiment(\"/Workspace/Users/c.palma@uniandes.edu.co/xgboost_gridsearch\")\n",
    "\n",
    "\n",
    "# Lista para guardar resultados y seleccionar la mejor combinación\n",
    "results = []\n",
    "\n",
    "# ==============================\n",
    "# 6. Grid Search con MLflow\n",
    "# ==============================\n",
    "for i, params in enumerate(param_combinations):\n",
    "    with mlflow.start_run(run_name=f\"run_{i}\") as run:\n",
    "        print(f\"\\n[LOG] Entrenando combinación {i+1}: {params}\")\n",
    "\n",
    "        # Definir modelo XGBoost\n",
    "        clf = XGBClassifier(\n",
    "            objective=\"multi:softmax\",\n",
    "            n_jobs=-1,\n",
    "            device=\"cuda\",\n",
    "            min_child_weight=4,\n",
    "            gamma=0.01,\n",
    "            reg_alpha=0,\n",
    "            reg_lambda=1,\n",
    "            **params\n",
    "        )\n",
    "\n",
    "        # Registrar parámetros en MLflow\n",
    "        mlflow.log_params(params)\n",
    "\n",
    "        # Entrenamiento\n",
    "        start_time = time.time()\n",
    "        clf.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=False)\n",
    "        elapsed_time = time.time() - start_time\n",
    "        mlflow.log_metric(\"training_time_sec\", elapsed_time)\n",
    "\n",
    "        # Evaluación usando recall_macro\n",
    "        y_pred = clf.predict(X_test)\n",
    "        recall = recall_score(y_test, y_pred, average=\"macro\")\n",
    "        mlflow.log_metric(\"recall_macro\", recall)\n",
    "\n",
    "        # Guardar modelo y label encoder\n",
    "        mlflow.xgboost.log_model(clf, \"xgboost_model\")\n",
    "        joblib.dump(label_encoder, \"label_encoder.pkl\")\n",
    "        mlflow.log_artifact(\"label_encoder.pkl\")\n",
    "\n",
    "        print(f\"[LOG] Combinación {i+1} terminada. Recall_macro={recall:.4f}, tiempo={elapsed_time:.2f}s\")\n",
    "\n",
    "        # Guardar resultados para análisis posterior\n",
    "        results.append({\"params\": params, \"recall\": recall, \"training_time_sec\": elapsed_time})\n",
    "\n",
    "# ==============================\n",
    "# 7. Seleccionar mejor combinación\n",
    "# ==============================\n",
    "best_run = max(results, key=lambda x: x[\"recall\"])\n",
    "print(\"\\n[LOG] Mejor combinación encontrada:\")\n",
    "print(\"Parámetros:\", best_run[\"params\"])\n",
    "print(\"Recall_macro:\", best_run[\"recall\"])\n",
    "print(\"Tiempo de entrenamiento (s):\", best_run[\"training_time_sec\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "09fc7512-79fa-4df2-aac7-4ae4b047264b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "GloveModel",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
